---
title: "Classifying Breast Cancer Tumors"
output: html_document
author: "Shodai Inose"
---
```{r, include = FALSE}
library(tidyverse)
library(purrr)
library(tidyr)
library(ggplot2)
library(scales)
library(caret)
library(multiUS)
library(glmnet)

bc = read.csv("./breast_cancer_wisconsin.data", encoding="UTF-16LE", header = FALSE)
names(bc) = c('ID', 'Clump Thickness', 'Uniformity of Cell Size', 'Uniformity of Cell Shape', 'Marginal Adhesion', 'Single Epithelial Cell Size', 'Bare Nuclei', 'Bland Chromatin', 'Normal Nucleoli', 'Mitoses', 'Class')

bc = bc %>% 
  janitor::clean_names() %>% 
  dplyr::select(-id) %>%
  mutate(bare_nuclei = as.numeric(na_if(bare_nuclei, '?')))

bc$class = ifelse(bc$class == "2", 0, 1)
```
# Logistic Regression and KNN to Classify Tumors as Malignant or Benign

In this analysis, I will be performing logistic regression and k-nearest neighbors (classification) to identify if tumors in patients should be classified as malignant or benign based on measurements of several variables. The dataset, titled "Breast Cancer Wisconsin (Original)," comes from the University of California, Irvine Machine Learning Repository.^[https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Original%29]

The data contains variables to measure characteristics such as clump thickness, uniformity of cell size/shape, marginal adhesion, single epithelial cell size, bare nuclei, bland chromatin, normal nucleoli, and mitoses. The data also contains information that classifies tumors as malignant or benign. 

### Exploratory Data Analysis

There are `r bc %>% count()` observations in the data.

#### Checking NAs

The first step of this analysis will be to check for missing values.
```{r}
colSums(is.na(bc))
```

There are 16 missing values in the `bare_nuclei` column. As there is no indication of why there are missing values in the dataset, I will impute the missing values using the k-nearest neighbors algorithm.

```{r}
set.seed(2023)
bc = KNNimp(bc, k = 10, scale = TRUE, meth = "weighAvg", distData = NULL)
```

#### Graphing Independent Variables

In order to conduct exploratory data analysis, I will graph the distributions of the variables and check for any other issues present in the data.

```{r, message = FALSE}
bc %>% 
  dplyr::select(-"class") %>%
  keep(is.numeric) %>% 
  gather() %>% 
  ggplot(aes(value)) +
    facet_wrap(~ key, scales = "free") +
    geom_histogram()
```

None of the variables appear to be distributed normally, but this should not pose a problem for the logistic regression. 

#### Graphing Dependent Variable
```{r}
ggplot(bc, aes(x = factor(class), fill = class)) +  
  geom_bar(aes(y = (after_stat(count))/sum(after_stat(count)))) +
  scale_y_continuous(labels = percent) +
  labs(title = "Percent of Malignant and Benign", x = "Class", y = "Percent") +
  theme(legend.position = "none")
```

Plotting the class variable, there does not appear to be an issue with class imbalance.

#### Checking Correlation

The next step will be to assess correlation between variables in the data. A key assumption of logistic regression and knn requires that there is little multicollinearity present in the data. 

```{r, warning = FALSE}
corrplot::corrplot(cor(bc), addCoef.col = "White", number.cex = 0.8, number.digits = 1, diag = FALSE, bg = "grey", outline = "black", addgrid.col = "white", marc = c(1, 1, 1, 1))
```

```{css, echo=FALSE}
table {
  display: block;
  overflow-x: scroll;
}
```

```{r, fig.width = 5}
rstatix::cor_mat(bc) %>% 
  knitr::kable(digits = 3)
```

The correlation plot above indicates relatively high correlation between uniformity of cell size and uniformity of cell shape (a value of 0.91). However, using regularization methods in the logistic regression (LASSO and ridge shrinkage methods) should take care of the issues.

#### Normalization

Although the dataset appears to have variables on the same scale, I will be conducting normalization to ensure that all values fall between 0 and 1.

```{r}
normalize = function(x){
   (x - min(x))/(max(x) - min(x))
}

bc_no_class = dplyr::select(bc, -class)
bc_class = dplyr::select(bc, class)

bc_normalized = as.data.frame(lapply(bc_no_class, normalize))
bc = cbind(bc_normalized, bc_class)
```

#### Splitting Test and Training Data

```{r}
bc$class = as.factor(bc$class)
set.seed(2023)
bc$id = 1:nrow(bc)
train =  bc %>% dplyr::sample_frac(0.70)
test  =  dplyr::anti_join(bc, train, by = 'id')
train = train %>% dplyr::select(-id)
test = test %>% dplyr::select(-id)
```

I have assigned 70% (`r nrow(train)` observations) of the data to the training dataset and 30% (`r nrow(test)` observations) of the data to the test dataset.

## Building the Model

#### LASSO 

I will build a logistic regression model using 10-fold cross validation and the LASSO shrinkage method.

```{r}
ctrlspecs = trainControl(method = "cv", 
                          number = 10,
                          savePredictions = "all")

lambda_vector = exp(seq(5, -5, length = 100))

set.seed(2023)

logistic_lasso = train(class ~ ., 
                data = train,
                preProcess = c("center","scale"),
                method = "glmnet",
                tuneGrid = expand.grid(alpha = 1, lambda = lambda_vector),
                trControl = ctrlspecs)
```

The optimal lambda value for the LASSO regression gives us the following coefficients for the model:

```{r}
coef(logistic_lasso$finalModel, logistic_lasso$bestTune$lambda)
```

There appears to be a positive relationship between every dependent variable (removing the normal nucleoli variable) and malignant tumor status. The single epithelial cell size was determined to not have an influence on determining malignant tumors. This model has a `r round(max(logistic_lasso$results$Accuracy) * 100, 2)`% accuracy on the training data.

#### Ridge Regression

I will also build a logistic regression model using 10-fold cross validation and the ridge regression shrinkage method.

```{r}
set.seed(2023)
ridge = train(class ~ ., 
                data = train,
                preProcess = c("center","scale"),
                method = "glmnet",
                tuneGrid = expand.grid(alpha = 0, lambda = lambda_vector),
                trControl = ctrlspecs)
```

The optimal lambda value for the ridge model gives us the following coefficients:

```{r}
coef(ridge$finalModel, ridge$bestTune$lambda)
```

There appears to be a positive relationship between every dependent variable and malignant tumor status. This model has a `r round(max(ridge$results$Accuracy) * 100, 2)`% accuracy on the training data.

#### Elastic Net

I will also build a logistic regression model using 10-fold cross validation and the elastic net regression shrinkage method.

```{r}
alpha_vector = seq(0, 1, length = 20)
set.seed(2023)

elastic = train(class ~ ., 
                data = train,
                preProcess = c("center","scale"),
                method = "glmnet",
                tuneGrid = expand.grid(alpha = alpha_vector, lambda = lambda_vector),
                trControl = ctrlspecs)
```

The optimal alpha value is `r elastic$bestTune$alpha`. The optimal lambda value for the elastic net model gives us the following coefficients:

```{r}
coef(elastic$finalModel, elastic$bestTune$lambda)
```

There appears to be a positive relationship between every dependent variable and malignant tumor status. This model has a `r round(max(elastic$results$Accuracy) * 100, 2)`% accuracy on the training data.

#### K-Nearest Neighbors

I will also be running a k-nearest neighbors algorithm to see if it outperforms the logistic regression models. In order to deal with the multicollinearity issue in the data, I will be creating two separate knn models: one with uniformity in cell shape and one with uniformity in cell size These models will also be trained using 10-fold cross validation.

```{r}
train1 = train %>% dplyr::select(-uniformity_of_cell_size)
train2 = train %>% dplyr::select(-uniformity_of_cell_shape)

set.seed(2023)

knn_cell_shape = train(class ~ .,
             method     = "knn",
             tuneGrid   = expand.grid(k = 1:10),
             trControl  = ctrlspecs,
             metric     = "Accuracy",
             data       = train1)

knn_cell_size = train(class ~ .,
             method     = "knn",
             tuneGrid   = expand.grid(k = 1:10),
             trControl  = ctrlspecs,
             metric     = "Accuracy",
             data       = train2)
```

The optimal knn model (containing uniformity in cell shape) has an accuracy of `r round(max(knn_cell_shape$results$Accuracy) * 100, 2)`% in predicting tumor status on the training data. The optimal knn model (containing uniformity in cell size) has an accuracy of `r round(max(knn_cell_size$results$Accuracy) * 100, 2)`%

## Choosing a Final Model

```{r}
set.seed(2023)

resamp = resamples(
  list(
    enet = elastic, 
    lasso = logistic_lasso, 
    ridge = ridge, 
    knn_shape = knn_cell_shape,
    knn_size = knn_cell_size))

summary(resamp)

bwplot(resamp, metric = "Accuracy")
```

It appears that the elastic net logistic regression model has the highest accuracy, on average, which means I will be selecting this model going forward.

## Assessing Final Model Performance

Using the test data set aside earlier, the model's accuracy, precision, and recall can be assessed. 

```{r}
test_new = dplyr::select(test, -class)
testing_prediction = predict(elastic, newdata = test_new, se = T)

confusion = confusionMatrix(data = testing_prediction, reference = test$class)

confusion
```

According to the testing predictions, the elastic net logistic regression model has an accuracy of 94.76% in predicting tumor status. The model has a recall of 98.51% and a precision of 93.62%. This shows that the model has done fairly well in identifying if tumors are benign or malignant. 

## Citations:

This breast cancer databases was obtained from the University of Wisconsin Hospitals, Madison from Dr. William H. Wolberg.

1. O. L. Mangasarian and W. H. Wolberg: "Cancer diagnosis via linear programming", SIAM News, Volume 23, Number 5, September 1990, pp 1 & 18.

2. William H. Wolberg and O.L. Mangasarian: "Multisurface method of pattern separation for medical diagnosis applied to breast cytology", Proceedings of the National Academy of Sciences, U.S.A., Volume 87, December 1990, pp 9193-9196.

3. O. L. Mangasarian, R. Setiono, and W.H. Wolberg: "Pattern recognition via linear programming: Theory and application to medical diagnosis", in: "Large-scale numerical optimization", Thomas F. Coleman and Yuying Li, editors, SIAM Publications, Philadelphia 1990, pp 22-30.

4. K. P. Bennett & O. L. Mangasarian: "Robust linear programming discrimination of two linearly inseparable sets", Optimization Methods and Software 1, 1992, 23-34 (Gordon & Breach Science Publishers).

Aspects of the code were developed from "R for HR: An Introduction to Human Resource Analytics Using R"^[https://rforhr.com/] by David E. Caughlin.^[https://creativecommons.org/licenses/by-nc-sa/4.0/]
